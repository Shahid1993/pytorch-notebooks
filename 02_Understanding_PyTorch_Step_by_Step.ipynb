{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Understanding PyTorch with an example: a step-by-step tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Regression Problem\n",
    "\n",
    "![](https://miro.medium.com/max/188/1*a7_GUQQT5BjvAhh3qq0JwA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffle the indices\n",
    "idx  = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[:80]\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generate training and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((80, 1), (80, 1)), ((20, 1), (20, 1)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, y_train.shape), (x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "===\n",
    "**Four basic steps** you’d need to go through to compute it :\n",
    "Step 1: Compute the Loss\n",
    "---\n",
    "For a regression problem, the loss is given by the **Mean Square Error (MSE)**, that is, the average of all squared differences between labels (y) and predictions (a + bx).\n",
    "\n",
    "It is worth mentioning that, if we use **all points** in the training set (N) to compute the loss, we are performing a **batch gradient descent**. If we were to use a **single point** at each time, it would be a **stochastic gradient descent**. Anything else (n) **in-between 1 and N** characterizes a **mini-batch gradient descent**.\n",
    "\n",
    "![](https://miro.medium.com/max/345/1*7fmJUcQT578OBfX7Q8hluQ.png)\n",
    "\n",
    "Step 2: Compute the Gradients\n",
    "---\n",
    "A gradient is a **partial derivative** — why partial? Because one computes it with respect to **(w.r.t.) a single parameter**. We have two parameters, a and b, so we must compute two partial derivatives.\n",
    "\n",
    "**_A derivative tells you how much a given quantity changes when you slightly vary some other quantity. In our case, how much does our MSE loss change when we vary each one of our two parameters?_**\n",
    "\n",
    "![](https://miro.medium.com/max/850/1*YvTj1B-h1gzSI5F24OgrrA.png)\n",
    "\n",
    "Step 3: Update the Parameters\n",
    "---\n",
    "In the final step, we _**use the gradients to update the parameters**_. Since we are trying to _**minimize our losses**_, we _**reverse the sign of the gradient**_ for the update.\n",
    "There is still another parameter to consider: the _**learning rate**_, denoted by the Greek letter _**eta**_ (that looks like the letter _**n**_), which is the _**multiplicative factor**_ that we need to apply to the gradient for the parameter update.\n",
    "\n",
    "![](https://miro.medium.com/max/209/1*eWnUloBYcSNPRBzVcaIr1g.png)\n",
    "\n",
    "Step 4: Rinse and Repeat!\n",
    "---\n",
    "Now we use the **updated parameters** to go back to **Step 1** and restart the process.\n",
    "\n",
    "> An **_epoch is complete whenever every point has been already used for computing the loss_**. For **batch** gradient descent, this is trivial, as it uses all points for computing the loss — **one epoch is the same as one update**. For **stochastic gradient** descent, **one epoch means N updates**, while for **mini-batch (of size n), one epoch has N/n updates**.\n",
    "\n",
    "Repeating this process over and over, for many epochs, is, in a nutshell, **training a model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression in Numpy\n",
    "===\n",
    "For training a model, there are **two initialization steps**:\n",
    "\n",
    "1. **_Random initialization of parameters/weights_** (we have only two, a and b);\n",
    "2. **_Initialization of hyper-parameters_** (in our case, only learning rate and number of epochs);\n",
    "\n",
    "Make sure to always initialize your random seed to ensure **reproducibility** of your results. As usual, the random seed is [42](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)), the least random of all random seeds one could possibly choose :-)\n",
    "\n",
    "**For each epoch, there are four training steps:**\n",
    "\n",
    "- **Compute model’s predictions** — this is the **_forward pass_**;\n",
    "- **Compute the loss**, using predictions and and labels and the appropriate loss function for the task at hand;\n",
    "- **Compute the gradients for every parameter**;\n",
    "- **Update the parameters**;\n",
    "\n",
    "Just keep in mind that, if you don’t use batch gradient descent (our example does),you’ll have to write an **inner loop** to perform the **four training steps** for either each **individual point (stochastic)** or **n points (mini-batch)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[1.02354094] [1.96896411]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Initialize random parameters randomly\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(a, b)\n",
    "\n",
    "# Set the learning rate & No. of Epochs\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # compute model's predictions\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # compute loss\n",
    "    error = (y_train - yhat)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # compute the gradients for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # update the parameters using the gradient and learning rate\n",
    "    a -= lr * a_grad\n",
    "    b -= lr * b_grad\n",
    "    \n",
    "    \n",
    "print(a, b)\n",
    "    \n",
    "    \n",
    "# SANITY CHECK : do we get same results as our Gradient Descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch\n",
    "===\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*GbwKkmA0NdndXRhOOwNclA.jpeg)\n",
    "\n",
    "## Loading Data, Devices and CUDA\n",
    "\n",
    "- `from_numpy()` returns a **CPU tensor**.\n",
    "- `to()` sends your tensor to whatever **device** you specify, including your **GPU** (referred to as `cuda` or `cuda:0`).\n",
    "- `cuda.is_available()` to find out if you have a GPU at your disposal and set your device accordingly.\n",
    "- `float()` to cast it to a lower precision (32-bit float)\n",
    "- `numpy()` turns tensors back to Numpy arrays, provided you've CPU tensors\n",
    "- `cpu()` to  convert gpu tensors to cpu tensors\n",
    "- `requires_grad=True` to enable **gradient computation of tensors**\n",
    "\n",
    "**The `to(device)` \"shadows\" the gradient...**\n",
    "\n",
    "> **In PyTorch, every method that ends with an underscore (_) makes changes in-place**, meaning, they will modify the underlying variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor, 'torch.FloatTensor')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8258], requires_grad=True) tensor([2.4454], requires_grad=True)\n",
      "tensor([0.7831], requires_grad=True) tensor([-0.1181], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6303], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0035], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6303], requires_grad=True) tensor([-0.0035], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)\n",
    "\n",
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(a, b)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient...\n",
    "\n",
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8dfcbadd30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
